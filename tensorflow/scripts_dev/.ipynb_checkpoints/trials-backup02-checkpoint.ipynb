{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "import argparse\n",
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "#from tensorflow.examples.tutorials.mnist import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "class Flags:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.01\n",
    "        self.max_steps = 2000\n",
    "        self.hidden1 = 128\n",
    "        self.hidden2 = 32\n",
    "        self.batch_size = 100\n",
    "        #self.input_data_dir = '/tmp/tensorflow/mnist/input_data'\n",
    "        self.input_data_dir = '/notebooks/datasets/mnist'\n",
    "        self.log_dir = '/tmp/tensorflow/mnist/logs/fully_connected_feed'\n",
    "        self.fake_data = False\n",
    "        \n",
    "\n",
    "FLAGS = Flags()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def inference(images, hidden1_units, hidden2_units):\n",
    "    \"\"\"Build the MNIST model up to where it may be used for inference. \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, hidden1_units], stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(tf.truncated_normal([hidden1_units, hidden2_units],stddev=1.0 / math.sqrt(float(hidden1_units))), \n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]), name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden2_units, NUM_CLASSES], stddev=1.0 / math.sqrt(float(hidden2_units))), name='weights')\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "    return logits\n",
    "\n",
    "\n",
    "def calculate_loss(logits, labels):\n",
    "    \"\"\"Calculates the loss from the logits and the labels.\"\"\"\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "    return tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "    \"\"\"Sets up the training Ops. \"\"\"\n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\"\"\"\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: loss = 2.30 (0.142 sec)\n",
      "Step 100: loss = 2.14 (0.002 sec)\n",
      "Step 200: loss = 1.85 (0.002 sec)\n",
      "Step 300: loss = 1.60 (0.002 sec)\n",
      "Step 400: loss = 1.26 (0.002 sec)\n",
      "Step 500: loss = 0.82 (0.002 sec)\n",
      "Step 600: loss = 0.78 (0.002 sec)\n",
      "Step 700: loss = 0.72 (0.002 sec)\n",
      "Step 800: loss = 0.55 (0.002 sec)\n",
      "Step 900: loss = 0.48 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 47839  Precision @ 1: 0.8698\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4393  Precision @ 1: 0.8786\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 8775  Precision @ 1: 0.8775\n",
      "Step 1000: loss = 0.66 (0.003 sec)\n",
      "Step 1100: loss = 0.50 (0.076 sec)\n",
      "Step 1200: loss = 0.49 (0.002 sec)\n",
      "Step 1300: loss = 0.46 (0.002 sec)\n",
      "Step 1400: loss = 0.38 (0.002 sec)\n",
      "Step 1500: loss = 0.42 (0.002 sec)\n",
      "Step 1600: loss = 0.25 (0.003 sec)\n",
      "Step 1700: loss = 0.29 (0.002 sec)\n",
      "Step 1800: loss = 0.41 (0.002 sec)\n",
      "Step 1900: loss = 0.37 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 49331  Precision @ 1: 0.8969\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4514  Precision @ 1: 0.9028\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9021  Precision @ 1: 0.9021\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def placeholder_inputs(batch_size):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors. \"\"\"\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    return images_placeholder, labels_placeholder\n",
    "\n",
    "\n",
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "    \"\"\"Fills the feed_dict for training the given step.\"\"\"\n",
    "    images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,\n",
    "                                                 FLAGS.fake_data)\n",
    "    feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "    \"\"\"Runs one evaluation against the full epoch of data.\"\"\"\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "    num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set,images_placeholder,labels_placeholder)\n",
    "        true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "        precision = float(true_count) / num_examples\n",
    "    print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' % (num_examples, true_count, precision))\n",
    "    \n",
    "\n",
    "def run_training():\n",
    "    \"\"\"Train MNIST for a number of steps.\"\"\"\n",
    "    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        images_placeholder, labels_placeholder = placeholder_inputs(FLAGS.batch_size)\n",
    "\n",
    "        logits = inference(images_placeholder,\n",
    "                                 FLAGS.hidden1,\n",
    "                                 FLAGS.hidden2)\n",
    "        loss = calculate_loss(logits, labels_placeholder)\n",
    "        train_op = training(loss, FLAGS.learning_rate)\n",
    "        eval_correct = evaluation(logits, labels_placeholder)\n",
    "        summary = tf.summary.merge_all()\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        sess = tf.Session()\n",
    "        summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "        sess.run(init)\n",
    "\n",
    "        for step in xrange(FLAGS.max_steps):\n",
    "            start_time = time.time()\n",
    "            feed_dict = fill_feed_dict(data_sets.train,\n",
    "                                     images_placeholder,\n",
    "                                     labels_placeholder)\n",
    "            _, loss_value = sess.run([train_op, loss],\n",
    "                                   feed_dict=feed_dict)\n",
    "\n",
    "            duration = time.time() - start_time\n",
    "            if step % 100 == 0:\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "                summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                summary_writer.flush()\n",
    "\n",
    "            if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "                checkpoint_file = os.path.join(FLAGS.log_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_file, global_step=step)\n",
    "                print('Training Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        images_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        data_sets.train)\n",
    "                print('Validation Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        images_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        data_sets.validation)\n",
    "                print('Test Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct,\n",
    "                        images_placeholder,\n",
    "                        labels_placeholder,\n",
    "                        data_sets.test)\n",
    "                \n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## extracting code from functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: loss = 2.30 (0.010 sec)\n",
      "Step 100: loss = 2.17 (0.002 sec)\n",
      "Step 200: loss = 1.90 (0.002 sec)\n",
      "Step 300: loss = 1.58 (0.002 sec)\n",
      "Step 400: loss = 1.31 (0.002 sec)\n",
      "Step 500: loss = 0.86 (0.002 sec)\n",
      "Step 600: loss = 0.79 (0.002 sec)\n",
      "Step 700: loss = 0.61 (0.002 sec)\n",
      "Step 800: loss = 0.58 (0.002 sec)\n",
      "Step 900: loss = 0.78 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 46705  Precision @ 1: 0.8492\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4292  Precision @ 1: 0.8584\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 8569  Precision @ 1: 0.8569\n",
      "Step 1000: loss = 0.56 (0.003 sec)\n",
      "Step 1100: loss = 0.55 (0.078 sec)\n",
      "Step 1200: loss = 0.44 (0.002 sec)\n",
      "Step 1300: loss = 0.65 (0.002 sec)\n",
      "Step 1400: loss = 0.41 (0.003 sec)\n",
      "Step 1500: loss = 0.40 (0.002 sec)\n",
      "Step 1600: loss = 0.47 (0.002 sec)\n",
      "Step 1700: loss = 0.37 (0.002 sec)\n",
      "Step 1800: loss = 0.60 (0.002 sec)\n",
      "Step 1900: loss = 0.38 (0.002 sec)\n",
      "Training Data Eval:\n",
      "  Num examples: 55000  Num correct: 49197  Precision @ 1: 0.8945\n",
      "Validation Data Eval:\n",
      "  Num examples: 5000  Num correct: 4511  Precision @ 1: 0.9022\n",
      "Test Data Eval:\n",
      "  Num examples: 10000  Num correct: 9025  Precision @ 1: 0.9025\n"
     ]
    }
   ],
   "source": [
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "    \"\"\"Runs one evaluation against the full epoch of data.\"\"\"\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "    num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set,images_placeholder,labels_placeholder)\n",
    "        true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "        precision = float(true_count) / num_examples\n",
    "    print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' % (num_examples, true_count, precision))\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    \n",
    "with tf.Graph().as_default():\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors. \"\"\"\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size))\n",
    "    \n",
    "    \"\"\"Build the MNIST model up to where it may be used for inference. \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights1 = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, FLAGS.hidden1], stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "            name='weights')\n",
    "        biases1 = tf.Variable(tf.zeros([FLAGS.hidden1]),name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights1) + biases1)\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights2 = tf.Variable(tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))), \n",
    "            name='weights')\n",
    "        biases2 = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights3 = tf.Variable(\n",
    "            tf.truncated_normal([FLAGS.hidden2, NUM_CLASSES], stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "        biases3 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases')\n",
    "        logits = tf.matmul(hidden2, weights3) + biases3\n",
    "        \n",
    "    \"\"\"Calculates the loss from the logits and the labels.\"\"\"\n",
    "    labels = tf.to_int64(labels_placeholder)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    \n",
    "    \"\"\"Sets up the training Ops. \"\"\"\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\"\"\"\n",
    "    correct = tf.nn.in_top_k(logits, labels_placeholder, 1)\n",
    "    eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "    \n",
    "    \"\"\"Prepare session, dataset and environment\"\"\"\n",
    "    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "    summary = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \"\"\"Train\"\"\"\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "        start_time = time.time()\n",
    "        images_feed, labels_feed = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "        feed_dict = {\n",
    "          images_placeholder: images_feed,\n",
    "          labels_placeholder: labels_feed,\n",
    "        }\n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "            \n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "            checkpoint_file = os.path.join(FLAGS.log_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "            \"\"\"I kept the do_eval() function cause I don't see the point in rewriting the same stuff again and gain so...\"\"\"\n",
    "            print('Training Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.train)\n",
    "            print('Validation Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.validation)\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gradients analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights1 [784, 128]\n",
      "biases1 [128]\n",
      "hidden1 [100, 128]\n",
      "weights2 [128, 32]\n",
      "biases2 [32]\n",
      "hidden2 [100, 32]\n",
      "6\n",
      "[(u'hidden1/weights:0', [784, 128]), (u'hidden1/biases:0', [128]), (u'hidden2/weights:0', [128, 32]), (u'hidden2/biases:0', [32]), (u'softmax_linear/weights:0', [32, 10]), (u'softmax_linear/biases:0', [10])]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "exceptions must be old-style classes or derived from BaseException, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-2d20888f728a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mtrain_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;34m\"\"\"Evaluate the quality of the logits at predicting the label.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: exceptions must be old-style classes or derived from BaseException, not NoneType"
     ]
    }
   ],
   "source": [
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "    \"\"\"Runs one evaluation against the full epoch of data.\"\"\"\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "    num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set,images_placeholder,labels_placeholder)\n",
    "        true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "        precision = float(true_count) / num_examples\n",
    "    print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' % (num_examples, true_count, precision))\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    \n",
    "with tf.Graph().as_default():\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors. \"\"\"\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size))\n",
    "    \n",
    "    \"\"\"Build the MNIST model up to where it may be used for inference. \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights1 = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, FLAGS.hidden1], stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "            name='weights')\n",
    "        print(\"weights1\", weights1.get_shape().as_list())\n",
    "        biases1 = tf.Variable(tf.zeros([FLAGS.hidden1]),name='biases')\n",
    "        print(\"biases1\", biases1.get_shape().as_list())\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights1) + biases1)\n",
    "        print(\"hidden1\", hidden1.get_shape().as_list())\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights2 = tf.Variable(tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))), \n",
    "            name='weights')\n",
    "        print(\"weights2\", weights2.get_shape().as_list())\n",
    "        biases2 = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "        print(\"biases2\", biases2.get_shape().as_list())\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "        print(\"hidden2\", hidden2.get_shape().as_list())\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights3 = tf.Variable(\n",
    "            tf.truncated_normal([FLAGS.hidden2, NUM_CLASSES], stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "        biases3 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases')\n",
    "        logits = tf.matmul(hidden2, weights3) + biases3\n",
    "        \n",
    "    \"\"\"Calculates the loss from the logits and the labels.\"\"\"\n",
    "    labels = tf.to_int64(labels_placeholder)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    \n",
    "    \"\"\"Sets up the training Ops. \"\"\"\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    #train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    grads_and_vars = optimizer.compute_gradients(loss)\n",
    "    print(len(grads_and_vars))\n",
    "    print( [ (x[1].name, x[0].get_shape().as_list()) for x in grads_and_vars])\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars)\n",
    "    \n",
    "    raise\n",
    "    \n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\"\"\"\n",
    "    correct = tf.nn.in_top_k(logits, labels_placeholder, 1)\n",
    "    eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "    \n",
    "    \"\"\"Prepare session, dataset and environment\"\"\"\n",
    "    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "    summary = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \"\"\"Train\"\"\"\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "        start_time = time.time()\n",
    "        images_feed, labels_feed = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "        feed_dict = {\n",
    "          images_placeholder: images_feed,\n",
    "          labels_placeholder: labels_feed,\n",
    "        }\n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "            \n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "            checkpoint_file = os.path.join(FLAGS.log_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "            \"\"\"I kept the do_eval() function cause I don't see the point in rewriting the same stuff again and gain so...\"\"\"\n",
    "            print('Training Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.train)\n",
    "            print('Validation Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.validation)\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## decoupling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "    \"\"\"Runs one evaluation against the full epoch of data.\"\"\"\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "    num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set,images_placeholder,labels_placeholder)\n",
    "        true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "        precision = float(true_count) / num_examples\n",
    "    print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' % (num_examples, true_count, precision))\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    \n",
    "with tf.Graph().as_default():\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors. \"\"\"\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size))\n",
    "    \n",
    "    \"\"\"Build the MNIST model up to where it may be used for inference. \"\"\"\n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights1 = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, FLAGS.hidden1], stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "            name='weights')\n",
    "        biases1 = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights1) + biases1)\n",
    "    \n",
    "        \n",
    "    \"\"\"ACTUAL DECOUPLING\"\"\"\n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights2 = tf.Variable(tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))), \n",
    "            name='weights')\n",
    "        biases2 = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "        #hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "        \n",
    "        # we set a new placeholder, this will be used to inject the output of hidden1 (message) into hidden2:\n",
    "        hidden1_shape = hidden1.get_shape()#.as_list()\n",
    "        layer2_input = tf.placeholder(tf.float32, shape=hidden1_shape)\n",
    "        hidden2 = tf.nn.relu(tf.matmul(layer2_input, weights2) + biases2)\n",
    "        \n",
    "    \"\"\"optimizer\"\"\"\n",
    "    optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "    \n",
    "    \"\"\"synthetic gradients model\"\"\"\n",
    "    with tf.name_scope('synthetic_gradients'):\n",
    "        # synthetic gradients model input\n",
    "        synthetic_gradients_model_input = tf.concat([\n",
    "            tf.reshape(hidden1,[-1]), # Hidden 1 output (message)\n",
    "            tf.reshape(weights2,[-1]), # Hidden 2 weights (layer state)\n",
    "            tf.reshape(biases2,[-1]), # Hidden 2 biases (layer state)\n",
    "            tf.reshape(tf.to_float(labels_placeholder), [-1]) # labels (c parameter)\n",
    "        ],0)\n",
    "\n",
    "        #synthetic_gradients_model_input_lenght = synthetic_gradients_model_input.get_shape().as_list()[0]\n",
    "        #synthetic_gradients_model_weights = tf.Variable(tf.truncated_normal([synthetic_gradients_model_input_lenght, 1],\n",
    "        #    name='weights')\n",
    "        synthetic_gradients_model_input_shape = synthetic_gradients_model_input.get_shape()#.as_list()[0]\n",
    "        synthetic_gradients_model_weights = tf.Variable(tf.truncated_normal(synthetic_gradients_model_input_shape,name='weights'))\n",
    "        synthetic_gradients_model_biases = tf.Variable(tf.zeros([FLAGS.synthetic_gradients_model_input_lenght]), name='biases')\n",
    "        synthetic_gradients_model_logit = tf.matmul(synthetic_gradients_model_input, synthetic_gradients_model_weights) \n",
    "                                                        + synthetic_gradients_model_biases\n",
    "            \n",
    "        #layer1 gradients\n",
    "        hidden1_model = [weights1, biases1]\n",
    "        hidden1_gradients = tf.gradients(hidden1, hidden1_model, grad_ys=synthetic_gradients_model_logit)\n",
    "        optimize_layer1 = optimizer.apply_gradients(zip(hidden1_gradients, hidden1_model))\n",
    "        \n",
    "        #CONTINUARE DA QUI\n",
    "        \n",
    "        #synthetic_gradients_model gradients\n",
    "        \n",
    "    \n",
    "    #def layer1_forward_and_backward_pass(sess, feed_dict):\n",
    "    #    synthetic_gradient = sess.run(synthetic_gradients_model_logit, feed_dict)\n",
    "        \n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights3 = tf.Variable(\n",
    "            tf.truncated_normal([FLAGS.hidden2, NUM_CLASSES], stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "        biases3 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases')\n",
    "        logits = tf.matmul(hidden2, weights3) + biases3\n",
    "        \n",
    "    \"\"\"Calculates the loss from the logits and the labels.\"\"\"\n",
    "    labels = tf.to_int64(labels_placeholder)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    \n",
    "    \n",
    "    \"\"\"Sets up the training Ops. \"\"\"\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    #train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    hidden2_gradients = optimizer.compute_gradients(loss, var_list=[layer2_input])[0][1]\n",
    "    if hidden2_gradients is None:\n",
    "        print('WARNING! hidden2_gradients is None')\n",
    "    \n",
    "    synthetic_gradients_model_loss = tf.abs(tf.sub(hidden2_gradients, hidden1_gradients))\n",
    "    \n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\"\"\"\n",
    "    correct = tf.nn.in_top_k(logits, labels_placeholder, 1)\n",
    "    eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "    \n",
    "    \"\"\"Prepare session, dataset and environment\"\"\"\n",
    "    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "    summary = tf.summary.merge_all()\n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.Session()\n",
    "    summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    \"\"\"Train\"\"\"\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "        start_time = time.time()\n",
    "        images_feed, labels_feed = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "        feed_dict = {\n",
    "          images_placeholder: images_feed,\n",
    "          labels_placeholder: labels_feed,\n",
    "        }\n",
    "        _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            summary_writer.flush()\n",
    "            \n",
    "        if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "            checkpoint_file = os.path.join(FLAGS.log_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "            \"\"\"I kept the do_eval() function cause I don't see the point in rewriting the same stuff again and gain so...\"\"\"\n",
    "            print('Training Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.train)\n",
    "            print('Validation Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.validation)\n",
    "            print('Test Data Eval:')\n",
    "            do_eval(sess,\n",
    "                    eval_correct,\n",
    "                    images_placeholder,\n",
    "                    labels_placeholder,\n",
    "                    data_sets.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## decoupling layers (step by step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 128]\n",
      "[128, 32]\n",
      "[32]\n",
      "[100, 10]\n",
      "[100, 138]\n",
      "hidden_2_flattened_model\n",
      "[4128]\n",
      "hidden_2_replicated_model\n",
      "[100, 4128]\n",
      "synthetic_gradients_model_input\n",
      "[100, 4266]\n",
      "[100, 12800]\n",
      "[100, 128]\n",
      "synthetic_gradients_model_loss Tensor(\"synthetic_gradients/Sqrt:0\", shape=(), dtype=float32)\n",
      "[(u'hidden1/weights:0', <tf.Tensor 'synthetic_gradients/gradients_1/hidden1/MatMul_grad/tuple/control_dependency_1:0' shape=(784, 128) dtype=float32>), (u'hidden1/biases:0', <tf.Tensor 'synthetic_gradients/gradients_1/hidden1/add_grad/tuple/control_dependency_1:0' shape=(128,) dtype=float32>), (u'hidden2/weights:0', None), (u'hidden2/biases:0', None), (u'synthetic_gradients/weights:0', None), (u'synthetic_gradients/biases:0', None)]\n",
      "downloading data sets\n",
      "Extracting /notebooks/datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /notebooks/datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /notebooks/datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /notebooks/datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "hidden2_true_gradients Tensor(\"gradients/hidden2/MatMul_grad/MatMul:0\", shape=(100, 128), dtype=float32)\n",
      "(100, 128)\n",
      "[[ 0.          0.          0.         ...,  0.          0.          0.12409036]\n",
      " [ 0.          0.54601038  0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.34968308  0.         ...,  0.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.33932146  0.39046291  0.         ...,  0.          0.          0.        ]\n",
      " [ 0.          0.28269097  0.         ...,  0.          0.03586955  0.        ]\n",
      " [ 0.          0.19100545  0.         ...,  0.          0.          0.        ]]\n",
      "net loss: 2.30754\n",
      "synthetic_gradients_model loss: 522.029\n",
      "coherence_asserion: [array([[ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       [False,  True,  True, ...,  True,  True,  True],\n",
      "       ..., \n",
      "       [False,  True,  True, ...,  True,  True,  True],\n",
      "       [False,  True,  True, ...,  True,  True,  True],\n",
      "       [False,  True,  True, ...,  True,  True,  True]], dtype=bool)]\n",
      "coherence_asserion: False\n",
      "coherence_asserion2: [array([[ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       ..., \n",
      "       [False,  True, False, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True]], dtype=bool)]\n",
      "coherence_asserion2: False\n"
     ]
    }
   ],
   "source": [
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            data_set):\n",
    "    \"\"\"Runs one evaluation against the full epoch of data.\"\"\"\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "    num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set,images_placeholder,labels_placeholder)\n",
    "        true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "        precision = float(true_count) / num_examples\n",
    "    print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' % (num_examples, true_count, precision))\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "if tf.gfile.Exists(FLAGS.log_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    \n",
    "with tf.Graph().as_default():\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors. \"\"\"\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(FLAGS.batch_size, IMAGE_PIXELS), name=\"net_input\")\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(FLAGS.batch_size), name=\"net_labels\")\n",
    "    \n",
    "    #optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(FLAGS.learning_rate)\n",
    "    \n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights1 = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, FLAGS.hidden1], stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "            name='weights')\n",
    "        biases1 = tf.Variable(tf.zeros([FLAGS.hidden1]), name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images_placeholder, weights1) + biases1)\n",
    "        \n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights2 = tf.Variable(tf.truncated_normal([FLAGS.hidden1, FLAGS.hidden2],stddev=1.0 / math.sqrt(float(FLAGS.hidden1))), \n",
    "            name='weights')\n",
    "        biases2 = tf.Variable(tf.zeros([FLAGS.hidden2]), name='biases')\n",
    "        #hidden2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "        \n",
    "        # we set a new placeholder, this will be used to inject the output of hidden1 (message) into hidden2:\n",
    "        hidden1_shape = hidden1.get_shape()#.as_list()\n",
    "        #print(hidden1_shape)\n",
    "        layer2_input = tf.placeholder(tf.float32, shape=hidden1_shape, name=\"layer2_input\")\n",
    "        hidden2 = tf.nn.relu(tf.matmul(layer2_input, weights2) + biases2)\n",
    "        #print(hidden2.get_shape())\n",
    "        \n",
    "    #synthetic gradients model\n",
    "    with tf.name_scope('synthetic_gradients'):\n",
    "        \n",
    "        def print_shape(t):\n",
    "            print(t.get_shape().as_list())\n",
    "            \n",
    "        print_shape(hidden1)\n",
    "        print_shape(weights2)\n",
    "        print_shape(biases2)\n",
    "        #print_shape(labels_placeholder)\n",
    "        \n",
    "        labels_one_hot = tf.one_hot(labels_placeholder, 10)\n",
    "        print_shape(labels_one_hot)\n",
    "        \n",
    "        input_and_labels = tf.concat([hidden1, labels_one_hot], 1)\n",
    "        print_shape(input_and_labels)\n",
    "        \n",
    "        hidden_2_flattened_model = tf.concat([\n",
    "            tf.reshape(weights2,[-1]), # Hidden 2 weights (layer state)\n",
    "            tf.reshape(biases2,[-1]), # Hidden 2 biases (layer state)\n",
    "        ],0)\n",
    "        print(\"hidden_2_flattened_model\")\n",
    "        print_shape(hidden_2_flattened_model)\n",
    "        \n",
    "        hidden_2_replicated_model = tf.concat([\n",
    "            tf.expand_dims(hidden_2_flattened_model,0) for _ in range(input_and_labels.get_shape().as_list()[0])\n",
    "        ],0)\n",
    "        print(\"hidden_2_replicated_model\")\n",
    "        print_shape(hidden_2_replicated_model)\n",
    "        \n",
    "        synthetic_gradients_model_input = tf.concat([\n",
    "            input_and_labels,\n",
    "            hidden_2_replicated_model\n",
    "        ],1)\n",
    "        print(\"synthetic_gradients_model_input\")\n",
    "        print_shape(synthetic_gradients_model_input)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # synthetic gradients model input\n",
    "        synthetic_gradients_model_input = tf.concat([\n",
    "            tf.reshape(hidden1,[-1]), # Hidden 1 output (message)\n",
    "            tf.reshape(weights2,[-1]), # Hidden 2 weights (layer state)\n",
    "            tf.reshape(biases2,[-1]), # Hidden 2 biases (layer state)\n",
    "            tf.reshape(tf.to_float(labels_one_hot), [-1]) # labels (c parameter)\n",
    "        ],0)\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(synthetic_gradients_model_input.get_shape())\n",
    "        \n",
    "        #weights1_length = 1\n",
    "        #for l in weights1.get_shape().as_list():\n",
    "        #    weights1_length *= l\n",
    "        \n",
    "        #biases1_length = 1\n",
    "        #for l in biases1.get_shape().as_list():\n",
    "        #    biases1_length *= l\n",
    "        \n",
    "        #print(weights1.get_shape().as_list())\n",
    "        #print(weights1_length)\n",
    "        \n",
    "        #print(biases1.get_shape().as_list())\n",
    "        #print(biases1_length)\n",
    "        \n",
    "        #hidden1_model_length = weights1_length + biases1_length\n",
    "        \n",
    "        hidden1_shape = hidden1.get_shape().as_list()\n",
    "        hidden1_output_length = 1\n",
    "        for n in hidden1_shape:\n",
    "            hidden1_output_length *= n\n",
    "        \n",
    "        synthetic_gradients_model_input_lenght = synthetic_gradients_model_input.get_shape().as_list()[1]\n",
    "        \n",
    "        \n",
    "        synthetic_gradients_model_weights = tf.Variable(\n",
    "            tf.truncated_normal([synthetic_gradients_model_input_lenght, hidden1_output_length]),name='weights')\n",
    "        synthetic_gradients_model_biases = tf.Variable(tf.zeros([hidden1_output_length]), name='biases')\n",
    "        synthetic_gradients_model_matmul = tf.matmul(synthetic_gradients_model_input, synthetic_gradients_model_weights)\n",
    "        synthetic_gradients_model_logits = synthetic_gradients_model_matmul + synthetic_gradients_model_biases\n",
    "        \n",
    "        \n",
    "        print_shape(synthetic_gradients_model_logits)\n",
    "        hidden1_synthetic_gradient = tf.reshape(tf.reduce_mean(synthetic_gradients_model_logits,0), hidden1_shape)\n",
    "        print_shape(hidden1_synthetic_gradient)\n",
    "        \n",
    "        \"\"\"synthetic_gradients_model_optimization\"\"\"\n",
    "        synthetic_gradients_model_train_input = tf.placeholder(tf.float32,\n",
    "                                                    shape=synthetic_gradients_model_input.get_shape().as_list(),\n",
    "                                                    name=\"train_input\")\n",
    "        synthetic_gradients_model_train_matmul = tf.matmul(synthetic_gradients_model_train_input, \n",
    "                                                            synthetic_gradients_model_weights)\n",
    "        synthetic_gradients_model_train_logits = synthetic_gradients_model_train_matmul + synthetic_gradients_model_biases\n",
    "        hidden1_synthetic_gradient_train = tf.reshape(tf.reduce_mean(synthetic_gradients_model_train_logits,0), hidden1_shape)\n",
    "        \n",
    "        true_gradients_placeholder = tf.placeholder(tf.float32, \n",
    "                                                    shape=(hidden1_synthetic_gradient_train.get_shape().as_list()), \n",
    "                                                    name=\"hidden2_true_gradients\")\n",
    "        synthetic_gradients_model_loss = tf.sqrt( tf.reduce_sum(tf.square(tf.subtract(\n",
    "                                                                                true_gradients_placeholder,\n",
    "                                                                                hidden1_synthetic_gradient_train))))\n",
    "        print(\"synthetic_gradients_model_loss\", synthetic_gradients_model_loss)\n",
    "                                    #reduction_indices=1))\n",
    "        synthetic_gradients_model_optimize_op = optimizer.minimize(synthetic_gradients_model_loss)\n",
    "        \n",
    "        \n",
    "        \"\"\"hidden1 optimizer\"\"\"\n",
    "        hidden1_grad_vars = optimizer.compute_gradients(hidden1, grad_loss=hidden1_synthetic_gradient)\n",
    "        hidden1_grad_vars_names = [ (x[1].name, x[0]) for x in hidden1_grad_vars ]\n",
    "        print(hidden1_grad_vars_names)\n",
    "        \n",
    "        hidden1_appliable_grads = [ t for t in hidden1_grad_vars if t[0] is not None]\n",
    "        hidden1_optimize = optimizer.apply_gradients(hidden1_appliable_grads)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    print(\"downloading data sets\")\n",
    "    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "    def hidden1_test():\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            images_feed, labels_feed = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "            feed_dict = {\n",
    "              images_placeholder: images_feed,\n",
    "              labels_placeholder: labels_feed,\n",
    "            }\n",
    "            hidden1_output_message, synt_grad, opt_output = sess.run(\n",
    "                [hidden1, hidden1_synthetic_gradient, hidden1_optimize], feed_dict=feed_dict)\n",
    "            print(hidden1_output_message)\n",
    "            print(synt_grad)\n",
    "            \n",
    "    #hidden1_test()\n",
    "        \n",
    "        \n",
    "        \n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights3 = tf.Variable(\n",
    "            tf.truncated_normal([FLAGS.hidden2, NUM_CLASSES], stddev=1.0 / math.sqrt(float(FLAGS.hidden2))), name='weights')\n",
    "        biases3 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases')\n",
    "        logits3 = tf.matmul(hidden2, weights3) + biases3\n",
    "        \n",
    "    \"\"\"Calculates the loss from the logits and the labels.\"\"\"\n",
    "    labels = tf.to_int64(labels_placeholder)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits3, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    \n",
    "    \"\"\"\n",
    "    hidden2_grad_vars = optimizer.compute_gradients(loss)\n",
    "    hidden2_grad_vars_names = [ (x[1].name, x[0]) for x in hidden2_grad_vars ]\n",
    "    print(hidden2_grad_vars_names)\n",
    "    \"\"\"\n",
    "    \n",
    "    hidden2_true_gradients_list = tf.gradients(loss, layer2_input)\n",
    "    hidden2_true_gradients = hidden2_true_gradients_list[0]\n",
    "    print(\"hidden2_true_gradients\", hidden2_true_gradients)\n",
    "        \n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    \n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\"\"\"\n",
    "    correct = tf.nn.in_top_k(logits3, labels_placeholder, 1)\n",
    "    eval_correct = tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "    \n",
    "    \n",
    "    with tf.name_scope('coherence_safety_assertion'):\n",
    "        coherence_safety_assertion_input1 = tf.placeholder(tf.float32, \n",
    "                                                           shape=hidden1_synthetic_gradient.get_shape().as_list(),\n",
    "                                                          name=\"input1\")\n",
    "        coherence_safety_assertion_input2 = tf.placeholder(tf.float32, \n",
    "                                                           shape=hidden1_synthetic_gradient_train.get_shape().as_list(),\n",
    "                                                          name=\"input2\")\n",
    "        coherence_safety_assertion = tf.equal(coherence_safety_assertion_input1, coherence_safety_assertion_input2)\n",
    "    \n",
    "    def hidden2_test():\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            images_feed, labels_feed = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "            feed_dict = {\n",
    "              images_placeholder: images_feed,\n",
    "              labels_placeholder: labels_feed,\n",
    "            }\n",
    "            hidden1_output_message, synt_grad, opt_output = sess.run(\n",
    "                [hidden1, hidden1_synthetic_gradient, hidden1_optimize], feed_dict=feed_dict)\n",
    "            print(hidden1_output_message.shape)\n",
    "            print(hidden1_output_message)\n",
    "            print(synt_grad)\n",
    "            \n",
    "            feed_dict2 = {\n",
    "              layer2_input: hidden1_output_message,\n",
    "              labels_placeholder: labels_feed,\n",
    "            }\n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict=feed_dict2)\n",
    "            print(loss_value)\n",
    "            \n",
    "    #hidden2_test()\n",
    "    \n",
    "    def synthetic_gradients_model_test():\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            images_feed, labels_feed = data_sets.train.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "            feed_dict = {\n",
    "              images_placeholder: images_feed,\n",
    "              labels_placeholder: labels_feed\n",
    "            }\n",
    "            \n",
    "            \"\"\"\n",
    "            hidden1 training step:\n",
    "            inputs flow from the beginning to the end of hidden1\n",
    "            synthetic_gradients_model generates a synthetic gradient for hidden1\n",
    "            the synthetic gradient gets applied by the optimizer and hidden1 weights get updated\n",
    "            \"\"\"\n",
    "            hidden1_output_message, synt_grad_input, synt_grad, opt_output = sess.run(\n",
    "                [hidden1, synthetic_gradients_model_input, hidden1_synthetic_gradient, hidden1_optimize], \n",
    "                feed_dict=feed_dict)\n",
    "            print(hidden1_output_message.shape)\n",
    "            print(hidden1_output_message)\n",
    "            #print(synt_grad)\n",
    "            \n",
    "            \"\"\"\n",
    "            hidden2 training step:\n",
    "            inputs flow from the beginning of hidden2 to the end of hidden3, input is the output of hidden1\n",
    "            optimizer minimizes the loss and updates hidden2 and hidden3 weights\n",
    "            \"\"\"\n",
    "            feed_dict2 = {\n",
    "              layer2_input: hidden1_output_message,\n",
    "              labels_placeholder: labels_feed\n",
    "            }\n",
    "            _, loss_value, true_gradients = sess.run([train_op, loss, hidden2_true_gradients], feed_dict=feed_dict2)\n",
    "            print(\"net loss:\", loss_value)\n",
    "            \n",
    "            \"\"\"\n",
    "            synthetic_gradients_model training step\n",
    "            \"\"\"\n",
    "            feed_dict3 = {\n",
    "              synthetic_gradients_model_train_input: synt_grad_input,\n",
    "              true_gradients_placeholder: true_gradients\n",
    "            }\n",
    "            _, synthetic_gradients_model_loss_value, synt_grad_train= sess.run(\n",
    "                [synthetic_gradients_model_optimize_op, synthetic_gradients_model_loss, hidden1_synthetic_gradient_train], \n",
    "                                    feed_dict=feed_dict3)\n",
    "            print(\"synthetic_gradients_model loss:\", synthetic_gradients_model_loss_value)\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            coherence_safety_assertion\n",
    "            \"\"\"\n",
    "            feed_dict4 = {\n",
    "              coherence_safety_assertion_input1: synt_grad,\n",
    "              coherence_safety_assertion_input2: synt_grad_train\n",
    "            }\n",
    "            assertion_result = sess.run([coherence_safety_assertion], feed_dict=feed_dict4)\n",
    "            \"\"\"\n",
    "            result = True\n",
    "            for x in assertion_result:\n",
    "                for y in x:\n",
    "                    result = result and y\n",
    "            \"\"\"\n",
    "            print(\"coherence_asserion:\", assertion_result)\n",
    "            print(\"coherence_asserion:\", np.asarray(assertion_result).all())\n",
    "            \n",
    "            synt_grad_train1 = sess.run(hidden1_synthetic_gradient_train, \n",
    "                                        feed_dict={synthetic_gradients_model_train_input: synt_grad_input})\n",
    "            synt_grad_train2 = sess.run(hidden1_synthetic_gradient_train, \n",
    "                                        feed_dict={synthetic_gradients_model_train_input: synt_grad_input})\n",
    "            \n",
    "            #print(synt_grad_train1[0].shape)\n",
    "            \n",
    "            feed_dict5 = {\n",
    "              coherence_safety_assertion_input1: synt_grad_train1,\n",
    "              coherence_safety_assertion_input2: synt_grad_train2\n",
    "            }\n",
    "            assertion_result = sess.run([coherence_safety_assertion], feed_dict=feed_dict5)\n",
    "            print(\"coherence_asserion2:\", assertion_result)\n",
    "            print(\"coherence_asserion2:\", np.asarray(assertion_result).all())\n",
    "            \n",
    "            \n",
    "            \n",
    "    synthetic_gradients_model_test()\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#synthetic_gradients_model_output = tf.reshape(synthetic_gradients_model_logits, hidden1_shape)\n",
    "            \n",
    "        #weights1_gradients = tf.reshape(tf.slice(synthetic_gradients_model_logit, [0], [weights1_length]), weights1.get_shape())\n",
    "        #biases1_gradients = tf.reshape(tf.slice(\n",
    "        #    synthetic_gradients_model_logit, [weights1_length], [weights1_length+biases1_length]), biases1.get_shape())\n",
    "        \n",
    "        \n",
    "            \n",
    "        #print(tf.reshape(weights1,[-1]).get_shape())\n",
    "        #layer1_lenght = tf.concat(tf.reshape(weights1,[-1]), tf.reshape(biases1[-1]))\n",
    "\n",
    "        \"\"\"\n",
    "        #synthetic_gradients_model_input_lenght = synthetic_gradients_model_input.get_shape().as_list()[0]\n",
    "        #synthetic_gradients_model_weights = tf.Variable(tf.truncated_normal([synthetic_gradients_model_input_lenght, 1],\n",
    "        #    name='weights')\n",
    "        synthetic_gradients_model_input_shape = synthetic_gradients_model_input.get_shape()#.as_list()[0]\n",
    "        synthetic_gradients_model_weights = tf.Variable(tf.truncated_normal(synthetic_gradients_model_input_shape,name='weights'))\n",
    "        synthetic_gradients_model_biases = tf.Variable(tf.zeros([FLAGS.synthetic_gradients_model_input_lenght]), name='biases')\n",
    "        synthetic_gradients_model_logit = tf.matmul(synthetic_gradients_model_input, synthetic_gradients_model_weights) \n",
    "                                                        + synthetic_gradients_model_biases\n",
    "            \n",
    "        #layer1 gradients\n",
    "        hidden1_model = [weights1, biases1]\n",
    "        hidden1_gradients = tf.gradients(hidden1, hidden1_model, grad_ys=synthetic_gradients_model_logit)\n",
    "        optimize_layer1 = optimizer.apply_gradients(zip(hidden1_gradients, hidden1_model))\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
